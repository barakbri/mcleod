---
title: "Monte Carlo for Estimating Latent Over-Dispersion"
author: "Daniel Yekutieli,Barak Brill"
date: "August 14, 2019"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true

vignette: |
    %\VignetteIndexEntry{mcleod}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mcleod)
```

# Introduction

Here we discuss the structure of the document, what the method is about and where it is applicable

# The generative model
   
   ![Diagram](./Diagram.png)
   
# Sampling from the posterior distribution
   
# Deconvolution estimates

## Deconvolution for binomial errors
```{r,cache=TRUE,eval=FALSE}
N = 30
K = 300
set.seed(1)
u = sample(c(0,1),size = K,replace = T)
x = rbinom(K,size = N,prob =inv.log.odds(rnorm(K,-1+3*u,sd = 0.3)))
n = rep(N,K)
head(cbind(x,n))
```

```{r,cache=TRUE,eval=FALSE}
res = mcleod(x, n)
  
plot.posterior(res)
```



```{r,cache=TRUE,eval=FALSE}
res = mcleod(x, n, a.limits = c(-5,5))
```


```{r,cache=TRUE,eval=FALSE}
prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.BETA.HEIRARCHICAL,
  Beta.Heirarchical.Levels = 6
  )
res = mcleod(x, n, prior_parameters = prior_obj)
```


```{r,cache=TRUE,eval=FALSE}
prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
  Two.Layer.Dirichlet.Intervals = 64,
  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8
  )

res = mcleod(x, n, prior_parameters = prior_obj)
```


```{r,cache=TRUE,eval=FALSE}
comp_obj = mcleod.computational.parameters(nr.gibbs = 500,
                                             nr.gibbs.burnin = 250)

res = mcleod(x, n,
             prior_parameters = prior_obj,
             computational_parameters = comp_obj)
```
## Deconvolution for Poisson errors

```{r,cache=TRUE,eval=FALSE}

  K = 200
  set.seed(1)
  u = sample(c(0,1),size = K,replace = T)
  x = rpois(K,lambda = exp(rnorm(K,2 + 3*u,0.5)) )
  
  res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),Noise_Type = MCLEOD.POISSON.ERRORS)
  
  plot.posterior(res)
```

Setting parameters...

```{r,cache=TRUE,eval=FALSE}

prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
  Two.Layer.Dirichlet.Intervals = 64,
  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8
  )

comp_obj = mcleod.computational.parameters(nr.gibbs = 500,
                                             nr.gibbs.burnin = 250)

res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),
             Noise_Type = MCLEOD.POISSON.ERRORS,
             prior_parameters = prior_obj,
             computational_parameters = comp_obj)

```
# Constructing confidence intervals for the CDF of rate parameters

## Method

![Diagram_2](./Diagram_2.png)

## Confidence intervals for CDF, binomial noise
```{r,cache=TRUE,eval=FALSE}
suppressWarnings(library(doRNG))

x.vec = deconvolveR::surg[,2]
n.vec = deconvolveR::surg[,1]

set.seed(1)

CI_obj= mcleod.CI.estimation.parameters(
                  Nr.reps.for.each.n = 1,
                  nr.cores = parallel::detectCores(),                                                                fraction.of.points.computed = 0.33,
                  epsilon.nr.gridpoints = 2)

prior_obj = mcleod.prior.parameters(
                  prior.type = MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
                  Two.Layer.Dirichlet.Intervals = 48,
                  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8)

CI.res = mcleod.estimate.CI(x.vec = x.vec,
                            n.vec = n.vec,
                            a.max = 3,
                            q_grid = seq(0.1,0.9,0.1),
                            CI.estimation.parameters = CI_obj,
                            prior_param = prior_obj,
                            verbose = F
                            )

plot.mcleod.CI(CI.res)

```

# GLMs with a random effect intercept of a general distribution

## Generative model
      
## Sampling from the posterior
      
## Logistic regression with a general random intercept

### Example 1: Coefficients intialized at 0
```{r,cache=TRUE,eval=FALSE}
N = 30
K = 200
set.seed(1)
covariates = matrix(rnorm(K*2,sd = 0.5),nrow = K)
colnames(covariates) = c('covariate 1','covariate 2')
real_beta_1 = -1
real_beta_2 = 1
x = rbinom(K,size = N,
           prob = inv.log.odds(rcauchy(K,location = 0,scale = 0.5) +
                     real_beta_1*covariates[,1] + real_beta_2*covariates[,2]))
n = rep(N,K)

head(round(cbind(x,n,covariates),digits = 2))
```

```{r,cache=TRUE,eval=FALSE}
res = mcleod(x, n, covariates = covariates)
```

```{r,cache=TRUE,eval=FALSE}
coeffs = mcleod::results.covariate.coefficients.posterior(res)
```

```{r,cache=TRUE,eval=FALSE}
print(coeffs)
```

```{r,cache=TRUE,eval=FALSE}
plot.posterior(res)
```

### Example 2: Coefficients intialized by logistic regression, change priors for $\beta$s

```{r,cache=TRUE,eval=FALSE}
  N = 30
  K = 300
  set.seed(2)
  covariates = matrix(rexp(K),nrow = K)
  real_beta = -0.5
  
  u = sample(c(0,1),size = K,replace = T)
  
  x = rbinom(K,size = N,
           prob =inv.log.odds(rnorm(K,-1+3*u,sd = 0.3) +
                                real_beta*covariates))
  n = rep(N,K)
```

```{r,cache=TRUE,eval=FALSE}
  model_dt = data.frame(c = x,nc = n-x)
  model_dt = cbind(model_dt,covariates)
  model <- glm(cbind(c,nc) ~.,family=binomial,data=model_dt)
  model$coefficients[-1]
```

```{r,cache=TRUE,eval=FALSE}
  coeffs_obj  = mcleod.covariates.estimation.parameters(
                  beta_init = model$coefficients[-1],
                  beta_prior_sd = c(1.5))

  res = mcleod(x,
               n,
               covariates = covariates,
               covariates_estimation_parameters = coeffs_obj
               )
  
```


```{r,cache=TRUE,eval=FALSE}
coeffs = mcleod::results.covariate.coefficients.posterior(res,plot.posterior = F)
print(coeffs)
```

```{r,cache=TRUE,eval=FALSE}
plot.posterior(res)
```

## Poisson regression with a general random intercept and offset term
```{r,cache=TRUE,eval=FALSE}

   
  K = 200
  set.seed(2)
  covariates = matrix(rexp(K,rate = 2),nrow = K)
  real_beta = 0.5
  u = sample(c(0,1),size = K,replace = T)
  extrinsic_size = runif(n = K,1,100)
  offset = log(extrinsic_size)
  
  x = rpois(K,lambda = extrinsic_size * exp(rnorm(K,2 + 3*u,0.5) + real_beta* covariates) )
  
  
  comp_obj = mcleod.computational.parameters(nr.gibbs = 1000,nr.gibbs.burnin = 500)
  
  res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),
               computational_parameters = comp_obj,
               covariates = covariates,
               Noise_Type = MCLEOD.POISSON.ERRORS,
               offset_vec = offset
               )
```

```{r,cache=TRUE,eval=FALSE}
mcleod::results.covariate.coefficients.posterior(res)

```

```{r,cache=TRUE,eval=FALSE}
plot.posterior(res)
```

## Setting the prior distributions on coefficient to be other than normal

```{r,cache=TRUE,eval=FALSE}
N = 30
K = 200
set.seed(1)
covariates = matrix(rnorm(K*2,sd = 0.5),nrow = K)
real_beta_1 = -1
real_beta_2 = 1
x = rbinom(K,size = N,prob = inv.log.odds(rnorm(K,0,sd = 1) + real_beta_1*covariates[,1] + real_beta_2*covariates[,2]))
n = rep(N,K)
```

```{r,cache=TRUE,eval=FALSE}
model_dt = data.frame(c = x,nc = n-x)
model_dt = cbind(model_dt,covariates)
model <- glm(cbind(c,nc) ~.,family=binomial,data=model_dt)
```


```{r, cache=TRUE,eval=FALSE}
beta_prior_points = seq(-5,5,0.01)

beta_prior_probs = pcauchy(beta_prior_points[-1]) - 
                  pcauchy(beta_prior_points[-length(beta_prior_points)])

beta_prior_probs = beta_prior_probs/ sum(beta_prior_probs)
  
```

```{r, cache=TRUE,eval=FALSE}
 plot(beta_prior_points[-1],beta_prior_probs,type = 'l',xlab = 'theta',ylab = 'probability in bin of beta_prior_probs')
```
 
```{r, cache=TRUE,eval=FALSE}
beta_prior_probs = matrix(c(beta_prior_probs, beta_prior_probs),ncol = 2)
```


```{r, cache=TRUE,eval=FALSE}
coeffs_obj = mcleod.covariates.estimation.parameters(
  beta_init = model$coefficients[-1],                                                                Manual_Prior_Values = beta_prior_points,                                                           Manual_Prior_Probs = beta_prior_probs)
```

```{r, cache=TRUE,eval=FALSE}
res = mcleod(x, n, covariates = covariates,
               covariates_estimation_parameters = coeffs_obj)
```
  
```{r, cache=TRUE,eval=FALSE}
mcleod::results.covariate.coefficients.posterior(res,plot.posterior = F)
```


# Additional topics

## changeing hyper-parameters for the priors of the mixing distribution:


```{r, cache=TRUE,eval=FALSE}

set.seed(1)

K = 2000 # Number of samples
n.vec = rep(2,K) #number of draws for each sample

# function for sampling a beta binomial (2,2)
generate_sample_Beta_2_2 = function(K){ 
  p.vec = rbeta(n = K,2,2)
  x.vec = rbinom(K,size = n.vec,p.vec)
  return(x.vec)
}

x.vec = generate_sample_Beta_2_2(K)

#run sampler:
mcleod_res = mcleod(x.vec,n.vec,a.limits = c(-4,4),exact.numeric.integration = T,
                    prior_param = mcleod.prior.parameters(prior.type = MCLEOD.PRIOR.TYPE.BETA.HEIRARCHICAL,Beta.Heirarchical.Levels = 5),
                    computational_parameters = mcleod.computational.parameters(nr.gibbs = 2000,nr.gibbs.burnin = 1000))

mcleod::plot.posterior(mcleod_res)

```

```{r, cache=TRUE,eval=FALSE}

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#Example how to change hyper-parameters.
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#For, example, how to change all hyper parameters to 2
# We form two matrices, for $\alpha^{L}$ and $\alpha^{U}$.
# In each matrix, the [l,i] entry gives the appropriate hyper parameter for the ith node on the lth level.

L = 5 # number of levels
alpha_L = matrix(NA,nrow = L,ncol = 2^(L-1)) # hyper-parameter matrices 
alpha_U = matrix(NA,nrow = L,ncol = 2^(L-1))

for(l in 1:L){
  for(k in 1:2^(l-1)){
    alpha_L[l,k] = 2
    alpha_U[l,k] = 2
  }
}

# we pass the hyper-paramers to the mcleod.prior.parameters(...) function
mcleod_res = mcleod(x.vec,n.vec,
                    a.limits = c(-4,4),
                    exact.numeric.integration = T,
                    prior_param = mcleod.prior.parameters(prior.type = MCLEOD.PRIOR.TYPE.BETA.HEIRARCHICAL,
                                                          Beta.Heirarchical.Levels = L,
                                                          Prior_Hyper_Parameters_BetaH_L = alpha_L,
                                                          Prior_Hyper_Parameters_BetaH_U = alpha_U),
                    computational_parameters = mcleod.computational.parameters(nr.gibbs = 2000,
                                                                               nr.gibbs.burnin = 1000)
                    )

mcleod::plot.posterior(mcleod_res)

```

```{r, cache=TRUE,eval=FALSE}


Nodes_in_first_layer = 8
Total_nr_nodes = 64

Two_Level_Dirichlet_Tree_Hyperparameters = matrix(NA,nrow = 2,ncol = Total_nr_nodes)

Two_Level_Dirichlet_Tree_Hyperparameters[1,1:Nodes_in_first_layer] = 2
Two_Level_Dirichlet_Tree_Hyperparameters[2,1:Total_nr_nodes] = 2

# we pass the hyper-paramers to the mcleod.prior.parameters(...) function
mcleod_res = mcleod(x.vec,n.vec,
    a.limits = c(-4,4),
    exact.numeric.integration = T,
    prior_param = mcleod.prior.parameters(prior.type = MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
                              Two.Layer.Dirichlet.Intervals = Total_nr_nodes,
                              Two.Layer.Dirichlet.Nodes.in.First.Layer =  Nodes_in_first_layer,
                               Prior_Hyper_Parameters_2LDT = Two_Level_Dirichlet_Tree_Hyperparameters),
    computational_parameters = mcleod.computational.parameters(nr.gibbs = 2000,
                                                               nr.gibbs.burnin = 1000)
)

mcleod::plot.posterior(mcleod_res)

```
