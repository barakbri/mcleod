---
title: "Monte Carlo for Estimating Latent Over-Dispersion"
author: "Daniel Yekutieli,Barak Brill"
date: "August 14, 2019"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true

vignette: |
    %\VignetteIndexEntry{mcleod}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mcleod)
```

# Introduction

Here we discuss the structure of the document, what the method is about and where it is applicable

# The generative model
   
   
# Sampling from the posterior distribution
   
# Deconvolution estimates

## Deconvolution for binomial errors
```{r,cache=TRUE,eval=FALSE}
N = 30
K = 300
set.seed(1)
u = sample(c(0,1),size = K,replace = T)
x = rbinom(K,size = N,prob =inv.log.odds(rnorm(K,-1+3*u,sd = 0.3)))
n = rep(N,K)
head(cbind(x,n))
```

```{r,cache=TRUE,eval=FALSE}
res = mcleod(x, n)
  
plot.posterior(res)
```



```{r,cache=TRUE,eval=FALSE}
res = mcleod(x, n, a.limits = c(-5,5))
```


```{r,cache=TRUE,eval=FALSE}
prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.BETA.HEIRARCHICAL,
  Beta.Heirarchical.Levels = 6
  )
res = mcleod(x, n, prior_parameters = prior_obj)
```


```{r,cache=TRUE,eval=FALSE}
prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
  Two.Layer.Dirichlet.Intervals = 64,
  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8
  )

res = mcleod(x, n, prior_parameters = prior_obj)
```


```{r,cache=TRUE,eval=FALSE}
comp_obj = mcleod.computational.parameters(nr.gibbs = 500,
                                             nr.gibbs.burnin = 250)

res = mcleod(x, n,
             prior_parameters = prior_obj,
             computational_parameters = comp_obj)
```
## Deconvolution for Poisson errors

```{r,cache=TRUE,eval=FALSE}

  K = 200
  set.seed(1)
  u = sample(c(0,1),size = K,replace = T)
  x = rpois(K,lambda = exp(rnorm(K,2 + 3*u,0.5)) )
  
  res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),Noise_Type = MCLEOD.POISSON.ERRORS)
  
  plot.posterior(res)
```

Setting parameters...

```{r,cache=TRUE,eval=FALSE}

prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
  Two.Layer.Dirichlet.Intervals = 64,
  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8
  )

comp_obj = mcleod.computational.parameters(nr.gibbs = 500,
                                             nr.gibbs.burnin = 250)

res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),
             Noise_Type = MCLEOD.POISSON.ERRORS,
             prior_parameters = prior_obj,
             computational_parameters = comp_obj)

```
# Constructing confidence intervals for the CDF of rate parameters

## Confidence intervals for CDF, binomial noise
```{r,cache=TRUE,eval=FALSE}
suppressWarnings(library(doRNG))

x.vec = deconvolveR::surg[,2]
n.vec = deconvolveR::surg[,1]

set.seed(1)

CI_obj= mcleod.CI.estimation.parameters(
                  Nr.reps.for.each.n = 1,
                  nr.cores = parallel::detectCores(),                                                                fraction.of.points.computed = 0.33,
                  epsilon.nr.gridpoints = 2)

prior_obj = mcleod.prior.parameters(
                  prior.type = MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET,
                  Two.Layer.Dirichlet.Intervals = 48,
                  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8)

CI.res = mcleod.estimate.CI(x.vec = x.vec,
                            n.vec = n.vec,
                            a.max = 3,
                            q_grid = seq(0.1,0.9,0.1),
                            CI.estimation.parameters = CI_obj,
                            prior_param = prior_obj,
                            verbose = F
                            )

plot.mcleod.CI(CI.res)

```

# GLMs with a random effect intercept of a general distribution

## Generative model
      
## Sampling from the posterior
      
## Logistic regression with a general random intercept

### Example 1: Coefficients intialized at 0
```{r,cache=TRUE,eval=FALSE}
N = 30
K = 200
set.seed(1)
covariates = matrix(rnorm(K*2,sd = 0.5),nrow = K)
colnames(covariates) = c('covariate 1','covariate 2')
real_beta_1 = -1
real_beta_2 = 1
x = rbinom(K,size = N,
           prob = inv.log.odds(rcauchy(K,location = 0,scale = 0.5) +
                     real_beta_1*covariates[,1] + real_beta_2*covariates[,2]))
n = rep(N,K)

head(round(cbind(x,n,covariates),digits = 2))
```

```{r,cache=TRUE,eval=FALSE}
res = mcleod(x, n, covariates = covariates)
```

```{r,cache=TRUE,eval=FALSE}
coeffs = mcleod::results.covariate.coefficients.posterior(res)
```

```{r,cache=TRUE,eval=FALSE}
print(coeffs)
```

```{r,cache=TRUE,eval=FALSE}
plot.posterior(res)
```

### Example 2: Coefficients intialized by logistic regression, change priors for $\beta$s

```{r,cache=TRUE,eval=FALSE}
  N = 30
  K = 300
  set.seed(2)
  covariates = matrix(rexp(K),nrow = K)
  real_beta = -0.5
  
  u = sample(c(0,1),size = K,replace = T)
  
  x = rbinom(K,size = N,
           prob =inv.log.odds(rnorm(K,-1+3*u,sd = 0.3) +
                                real_beta*covariates))
  n = rep(N,K)
```

```{r,cache=TRUE,eval=FALSE}
  model_dt = data.frame(c = x,nc = n-x)
  model_dt = cbind(model_dt,covariates)
  model <- glm(cbind(c,nc) ~.,family=binomial,data=model_dt)
  model$coefficients[-1]
```

```{r,cache=TRUE,eval=FALSE}
  coeffs_obj  = mcleod.covariates.estimation.parameters(
                  beta_init = model$coefficients[-1],
                  beta_prior_sd = c(1.5))

  res = mcleod(x,
               n,
               covariates = covariates,
               covariates_estimation_parameters = coeffs_obj
               )
  
```


```{r,cache=TRUE,eval=FALSE}
coeffs = mcleod::results.covariate.coefficients.posterior(res,plot.posterior = F)
print(coeffs)
```

```{r,cache=TRUE,eval=FALSE}
plot.posterior(res)
```

## Poisson regression with a general random intercept
```{r,cache=TRUE}

  K = 200
  set.seed(2)
  covariates = matrix(rexp(K,rate = 2),nrow = K)
  real_beta = 0.5
  u = sample(c(0,1),size = K,replace = T)
  x = rpois(K,lambda = exp(rnorm(K,2 + 3*u,0.5) + real_beta* covariates) )
  
  comp_obj = mcleod.computational.parameters(nr.gibbs = 1000,nr.gibbs.burnin = 500)
  
  res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),
               computational_parameters = comp_obj,
               covariates = covariates,
               Noise_Type = MCLEOD.POISSON.ERRORS
               )
```

```{r,cache=TRUE,eval=FALSE}
mcleod::results.covariate.coefficients.posterior(res)

```

```{r,cache=TRUE,eval=FALSE}
plot.posterior(res)
```

## Setting the prior distributions on coefficient to be other than normal

```{r,cache=TRUE,eval=FALSE}
N = 30
K = 200
set.seed(1)
covariates = matrix(rnorm(K*2,sd = 0.5),nrow = K)
real_beta_1 = -1
real_beta_2 = 1
x = rbinom(K,size = N,prob = inv.log.odds(rnorm(K,0,sd = 1) + real_beta_1*covariates[,1] + real_beta_2*covariates[,2]))
n = rep(N,K)
```

```{r,cache=TRUE,eval=FALSE}
model_dt = data.frame(c = x,nc = n-x)
model_dt = cbind(model_dt,covariates)
model <- glm(cbind(c,nc) ~.,family=binomial,data=model_dt)
```


```{r, cache=TRUE,eval=FALSE}
beta_prior_points = seq(-5,5,0.01)

beta_prior_probs = pcauchy(beta_prior_points[-1]) - 
                  pcauchy(beta_prior_points[-length(beta_prior_points)])

beta_prior_probs = beta_prior_probs/ sum(beta_prior_probs)
  
```

```{r, cache=TRUE,eval=FALSE}
 plot(beta_prior_points[-1],beta_prior_probs,type = 'l',xlab = 'theta',ylab = 'probability in bin of beta_prior_probs')
```
 
```{r, cache=TRUE,eval=FALSE}
beta_prior_probs = matrix(c(beta_prior_probs, beta_prior_probs),ncol = 2)
```


```{r, cache=TRUE,eval=FALSE}
coeffs_obj = mcleod.covariates.estimation.parameters(
  beta_init = model$coefficients[-1],                                                                Manual_Prior_Values = beta_prior_points,                                                           Manual_Prior_Probs = beta_prior_probs)
```

```{r, cache=TRUE,eval=FALSE}
res = mcleod(x, n, covariates = covariates,
               covariates_estimation_parameters = coeffs_obj)
```
  
```{r, cache=TRUE,eval=FALSE}
mcleod::results.covariate.coefficients.posterior(res,plot.posterior = F)
```

# Interval censoring
```{r, cache=TRUE,eval=FALSE}
suppressWarnings(library(icenReg)) 
data(miceData)
  
  
miceData.mat	<- cbind(miceData$l,miceData$u)
fit.all <- ic_np(miceData.mat)
  
  
a	<- mcleod.interval.censoring.density.estimation(miceData.mat,
                                                  a.rng = c(0,1200),n.gbbs = 200)
  
plot.posterior.interval.censoring(a)
  
```
