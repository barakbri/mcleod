% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mcleod_main_function.R
\name{mcleod}
\alias{mcleod}
\title{Function estimates the mixing distribution, for data with Binomial/Poisson samples with varying success probabilities/rates.}
\usage{
mcleod(
  x.smp,
  n.smp,
  a.limits = c(-4, 4),
  Noise_Type = MCLEOD.BINOMIAL.ERRORS,
  covariates = NULL,
  prior_parameters = NULL,
  computational_parameters = NULL,
  covariates_estimation_parameters = NULL,
  input_P_k_i = NULL,
  exact.numeric.integration = TRUE,
  offset_vec = rep(0, length(x.smp)),
  nr_threads = 1
)
}
\arguments{
\item{x.smp}{a vector of N measurements. For the binomial sampling distribution, measurement must be smaller or equal to n.smp (entry-wise)}

\item{n.smp}{number of draws, per observation, for the binomial model. For poisson sampling errors, set this value to NULL.}

\item{a.limits}{a vector with two entries, setting the range for the support of log(p/(1-p)) or log(lambda) for the model with no covariates. When using a model with covariates, these two values set the range for the support of gamma.}

\item{Noise_Type}{Noise type: either \code{MCLEOD.BINOMIAL.ERRORS} or \code{MCLEOD.POISSON.ERRORS}}

\item{covariates}{(optional) a matrix of covariates. Must as have a number of rows equal to length(x.smp). Different colmn represent different covariates.}

\item{prior_parameters}{Result of \code{\link{mcleod.prior.parameters}}. Sets parameter for the mixing / random intercept distribution.}

\item{computational_parameters}{Result of \code{\link{mcleod.computational.parameters}} Sets computational parameters for the MCMC sampler used for estimating the mixing / random intercept distribution.}

\item{covariates_estimation_parameters}{Result of \code{\link{mcleod.covariates.estimation.parameters}}. Sets statistical and computational}

\item{input_P_k_i}{A parallel approach for specifying the sampling distribution (instead of using binomial/poisson sampling). This parameter is used to define a matrix specifying each observations probability of being sampled, from each segment of the prior. The rows of the matrix need to correspond to observations, and the columns need to be associated with the segments of the piece-wise constant mixing distribution. The number of segments is defined by the prior object given in the argument \code{prior_parameters}. For example, for the Heirarchical Beta (Polya tree) prior there are 2^(L-1) segments placed evenly in the range defined by the argument \code{a.limits}, see additional details in the function \code{\link{mcleod.prior.parameters}}}

\item{exact.numeric.integration}{should exact numeric integration be used. See additional details in the doc page for \code{\link{mcleod.prior.parameters}}.}

\item{offset_vec}{A vector of covariates added with a slope of 1 to the model. This can be used for example to add terms of the form "+log(N_i)" to a poisson model, in order to incorporate knowledge about the extensive size of the sampled units.}

\item{nr_threads}{The function allows multiple x.smp to be a list of data samples, with each entry of the list being a vector of the same size as \code{n.smp}. When x.smp is set to be a list of vectors, this parameter sets the number of threads used for parallel computation of the deconvolution estimates. There results are returned as a list: \code{original_stat_res} (described below) become a list of the deconvolution estimates for the different data samples (with entries corresponding to the entries of the list supplied under \code{x.vec}). If \code{x.smp} is a list, then \code{input_P_k_i} can also be a list (but not vice versa)}
}
\value{
an object of type 'mcleod.obj' with the two following lists.
A list named \code{parameters_list} with the following entries:
\itemize{
\item{a.vec}{ - The mixing distribution for the binomial/poisson samples is estimated as a piece-wise constant function, over these breaking point.}
\item{nr.gibbs}{ - Number of gibbs samples}
\item{nr.gibbs.burnin}{ - Number of gibbs samples taken as burnin (excluded when computing estimators)}
\item{prior_parameters}{ - Input object by same name.}
\item{computational_parameters}{ - Input object by same name.}
\item{covariates_estimation_parameters}{ - Input object by same name.}
\item{covariates}{ - The matrix of inserted covariates}
\item{x.smp}{ - Input data by same name}
\item{n.smp}{ - Input data by same name}
\item{Noise_Type}{ - 0 for binomial, 1 for Poisson.}
\item{covariates_given}{ - Logical value indicating if covariates were given.}
}

And a second list named \code{original_stat_res} with the following entries:
\itemize{
\item{p_k_i}{ - A matrix with dimensions (length(x.smp),length(a.vec)-1). Entries of the matrix are the probabilities of each observation (by row), to be sampled when the random effect is sampled uniformly from each of the segments of a.vec (segments corresponding to columns).}
\item{n_smp}{ - A matrix of size (length(a.vec)-1,nr.gibbs) giving for each MCMC iteration (by column), the number of observations generated from each segment of the support of the mixing distribution. }
\item{pi_smp}{ - A matrix of size (length(a.vec)-1,nr.gibbs) giving for each MCMC iteration (by column), a posterior sample of the mixing distribution. computing the mean of each row gives the posterior mean for each segment of the piecewise constant mixing distribution. }
\item{beta_smp}{ - When covariates are included, a matrix of size (ncol(covariates),nr.gibbs), giving the MCMC samples for the vector of slope coefficients.}
\item{beta_suggestion}{ - When covariates are included,a matrix of size (ncol(covariates),nr.gibbs), giving the MCMC proposals for the vector of slope coffeficients. At each iteration, the proposal is suggested based on a normally distributed step from the previous MCMC step, and chosen usin a Metropolis Hastings rule.}
\item{proposal_approved}{ - When covariates are included: for each step, was the proposal for the vector of slope coefficients approved. The parameters proposal_approved, ll_proposal and ll_current are used for interogating the acceptance rate for the sampler, when covariates are used.}
\item{elapsed_secs}{ - Running time in seconds.}
\item{ll_proposal}{ - When covariates are included: for each MCMC step, the loglikelihood value of proposal of the new parameter values. Values computed without the probability of the polya tree prior, see CPP code for details.}
\item{ll_current}{ - When covariates are included: for each MCMC step, the loglikelihood value of the current parameter values. Values computed without the probability of the polya tree prior, see CPP code for details.}
}
}
\description{
Given a dataset with either binomial or Poisson samples, the function estimates the mixing distribution of \eqn{P} or \eqn{\lambda}{lambda}, respectively. The function assumes the following hierarchical Bayesian model for the mixing distribution: For binomial measurements, the model is \eqn{X_i\sim bin(N,P_i)}{Xi~bin(N,Pi)}, where  \eqn{log(P_i/(1-P_i))}{log(Pi/(1-Pi))} is random deviate from a piecewise-constant function, whose density values are modeled using either a Polya-Tree or Dirichlet Tree distribution. For Poisson errors, the model equation is \eqn{X_i\sim Pois(lambda_i)}{X_i~Pois(lambda_i)}, where \eqn{log(lambda_i)} is modeled using the heirarchical Bayesian approach (using a piecewise-constant density function).
If covariates are included in the model, the model is \eqn{log(P_i/(1-P_i)) = \gamma_i + \vec{\beta}^T \vec{Z}_i}, where the density of \eqn{\gamma_i}'s is estimated using the heirarchical Bayesian approach, \eqn{\vec{\beta}} is a vector of slopes, and \eqn{\vec{Z}_i} is a vector of sample covariates. For Poisson errors, the model equation is \eqn{log(\lambda_i) = \gamma_i + \vec{\beta}^T \vec{Z}_i}.
}
\details{
The mixing distribution ( or the density of \eqn{\gamma_i}'s for the setting with covariates) is estimated using an MCMC sampler conditioning on the observed data. The hyperparameters for prior for the mixing distribution are defined using the function \code{\link{mcleod.prior.parameters}}, and passed as an object to the argument \code{prior_parameters}, together with the support for the mixing distribution (random intercept term for the case with covariates), and the number of breaks in the piecewise constant density function (discontinuity points in the density function are equally placed across the support).
The returned object contains both the parameters and data given by the user, together with results for the MCMC sampler. The main result returned by the function is \code{pi_smp}: a series of density functions sampled from the posterior distribution of the mixing distribution (density of \eqn{\gamma_i}'s for the case with covariates). The function \code{\link{plot.posterior}} plots the samples from the posterior distribution of the random effect. See the package vignette on how to extract results from the returned object.
See the package vignette and cited papers for additional information on the model and statistical estimation approach.

If covariates are also supplied by the user, the algorithm performs two types of MCMC steps: one for the density of the random intercept term (density of  \eqn{\gamma_i's}) and one for the vector of slope coefficients (given by \eqn{\vec{\beta}}). MCMC steps for the slope coefficients are done using an accept-reject Metropolis-Hastings step. The function \code{\link{results.covariate.coefficients.posterior}} allows the mean posterior estiamtes of \eqn{\vec{\beta}} to be extracted from the output, together with plots and statistics for the posterior distribution of \eqn{\vec{\beta}}, and the acceptance ratio of the MH step.
The prior distribution for \eqn{\vec{\beta}}, together with parameters for the proposal distribution (suggesting the MCMC algorithm new values of \eqn{\vec{\beta}} based on the current iteration's values) can be set using the parameter \code{covariates_estimation_parameters}.
}
\examples{
 # For full description of package model and workflow,
 # including this function, Type browseVignettes(package = 'mcleod') 
 # in the R console and check the package vignette

library(mcleod)
##################################
# Example 1: Binomial sampling distribution, default parameters:
##################################
N = 30
K = 300
set.seed(1)
u = sample(c(0,1),size = K,replace = T)
x = rbinom(K,size = N,prob =inv.log.odds(rnorm(K,-1+3*u,sd = 0.3)))
n = rep(N,K)

#fit model
res = mcleod(x, n)

#plot posterior mixing distribution:
posterior_mixing_dist = mcleod.get.posterior.mixing.dist(res)

#' x_points = seq(-5,5,0.01)
plot(x_points, posterior_mixing_dist$density(x_points),
type = 'l',xlab = 'theta',ylab = 'Density')

x_points = seq(-5,5,0.01)
plot(x_points, posterior_mixing_dist$CDF(x_points),
type = 'l',xlab = 'theta',ylab = 'Density')

# Plot CDF with distribution of posterior samples
plot.posterior(res)

#############################
# Example 2: changing parameters:
#############################
# Example 2.1 -  how to change the support of the mixing distribution:
res = mcleod(x, n, a.limits = c(-5,5))

# Example 2.2 -  how to change the number of levels for the Polya tree prior
prior_obj  = mcleod.prior.parameters( #construct object defining the prior
  prior.type =MCLEOD.PRIOR.TYPE.BETA.HEIRARCHICAL, #type of prior
  Beta.Heirarchical.Levels = 6
)

res = mcleod(x, n, prior_parameters = prior_obj) #pass object to main function as argument

# Example 2.3 -  how to change the prior to a 2-level Dirichlet tree
prior_obj  = mcleod.prior.parameters(
  prior.type =MCLEOD.PRIOR.TYPE.TWO.LAYER.DIRICHLET, # define a 2-layer Dirichlet tree
  Two.Layer.Dirichlet.Intervals = 64, #number of segments
  # Note: Two.Layer.Dirichlet.Intervals must
  # be an integer multiple of 
  #Two.Layer.Dirichlet.Nodes.in.First.Layer
  Two.Layer.Dirichlet.Nodes.in.First.Layer = 8
)

res = mcleod(x, n, prior_parameters = prior_obj) #pass as argument

# Example 2.4 -  how to change the number of MCMC samples, and burn-in length
comp_obj = mcleod.computational.parameters(
  nr.gibbs = 500, #define the number of iter.s
  nr.gibbs.burnin = 250) 

res = mcleod(x, n, computational_parameters = comp_obj) # pass object as argument

#############################
# Example 3: Estimating the mixing distribution with Poisson samples
#############################

#Generate data
K = 200 # number of samples
set.seed(1)
#u sets right or left component in the mix. dist. for each obs.:
u = sample(c(0,1),size = K,replace = T) 
x = rpois(K,lambda = exp(rnorm(K,2 + 3*u,0.5)) ) #sample the obs

# Fit model:
res = mcleod(x, n.smp = NULL,a.limits = c(-2,8),Noise_Type = MCLEOD.POISSON.ERRORS)

# Plot the CDF of the mixing distribution, toghether with posterior samples
plot.posterior(res)

#############################
# Example 4: Binomial regression with a random normal intercept
#############################
# Generate data:
N = 30 #Number of draws per binomial observations
K = 200 #Number of samples
set.seed(1)
covariates = matrix(rnorm(K*2,sd = 0.5),nrow = K) #Generate covariates
colnames(covariates) = c('covariate 1','covariate 2')
#define slopes:
real_beta_1 = -1
real_beta_2 = 1
#sample
x = rbinom(K,size = N,
prob = inv.log.odds(rcauchy(K,location = 0,scale = 0.5) +
real_beta_1*covariates[,1] + real_beta_2*covariates[,2]))
n = rep(N,K)

# Fit model:
res = mcleod(x, n, covariates = covariates)

#Plot posterior for coefficients:
coeffs = mcleod::results.covariate.coefficients.posterior(res)

# Posterior mean for coefficients:
coeffs$posterior.means

#Posterior distribution of random intercept:
plot.posterior(res)

#############################
# Example 5: Poisson regression with a random normal intercept and an offset term
#############################

# generate data
K = 200 #Number of samples
set.seed(2)
# A exponentially distributed covariate:
covariates = matrix(rexp(K,rate = 2),nrow = K) 
# Value of the slope coefficient:
real_beta = 0.5

#indicator for the component in the bimodel intercept distribution.
u = sample(c(0,1),size = K,replace = T) 
#draw a random intrinsic size, known to the user.
extrinsic_size = runif(n = K,1,100)
offset = log(extrinsic_size) #convert to log scale.

#generate data, note how the offset term affects the rate (offset is additive to log lambda):
x = rpois(K,
lambda = extrinsic_size * exp(rnorm(K,2 + 3*u,0.5) + real_beta* covariates)
)

#Plot data
hist(x,main='Dist. of Xi for Poisson regression with offset term',breaks = 50)

#set the number of iterations
comp_obj = mcleod.computational.parameters(nr.gibbs = 3000,nr.gibbs.burnin = 500)

#fit model
res = mcleod(x, n.smp = NULL, #n.smp is null for Pois regression
     a.limits = c(-2,8),
     computational_parameters = comp_obj,
     covariates = covariates, # pass covariates
     Noise_Type = MCLEOD.POISSON.ERRORS, #Poisson regression
     offset_vec = offset #pass offset term
)

# posterior dist of slope coefficients
coeffs = mcleod::results.covariate.coefficients.posterior(res)

# posterior mean
print(coeffs$posterior.means)

# posterior distribution of random intercepts:
posterior_dist_gamma = mcleod.get.posterior.mixing.dist(res)

x_points = seq(-2,8,0.01)
plot(x_points, posterior_mixing_dist$density(x_points),
type = 'l',xlab = 'theta',ylab = 'Density')
}
